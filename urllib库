1.	import urllib.request
	html=urllib.request.urlopen(url)   
	将网页写入本地文件：
	data=html.read() 读取全部内容，赋值给一个字符串变量  or   data=html.readline() 读取一行   or   data=html.readlines() 读取全部内容，赋给一个列表变量，若读取全部内容，建议用这个
	with open(file) as name:
		name.write(data)
	(file--新建.html格式，'wb'   name--随意取名)  file打开就是一个网页界面
	or
	urllib.request.urlretrieve(url,filename=file)
		此方法会造成一些缓存，可以清除
	urllib.request.urlcleanup()
	
	以下file，等同于上面的html
	file.info()	 获取当前环境有关信息
	file.getcode()	获取当前爬取网页的状态码，200为正确，其他为错误
	file.geturl()  获取当前所爬取的网页
	
2.	URL标准中只会允许一部分ASCII字符，其他的比如汉字，是不符合的，所以需要对它们进行编码
	urllib.request.quote()
	urllib.request.unquote()  进行解码
	
3.	浏览器的模拟
	import urllib.request
	url=''
	headers=('User-Agent','Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3314.0 Safari/537.36 SE 2.X MetaSr 1.0')
	opener=urllib.request.build_opener()
	opener.addheaders=[headers]
	data=opener.open(url).read()
	    or
	import urllib.request
	url=''
	req=urllib.request.Request(url)
	req.add_header('User-Agent','Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3314.0 Safari/537.36 SE 2.X MetaSr 1.0')
	data=urllib.request.urlopen(req).read()
	
4.	超时设置
	urllib.request.urlopen(url,timeout=)   timeout设置时间，秒为单位，超过这个数，会爬取报错
	ep:
	import urllib.request
	for i in range(100):
		try:
			data=urllib.request.urlopen(url,timeout=10).read()
			print(len(data))
		except Exception as e:
			print('出现错误:'+str(e))

5.	http协议请求
	GET请求：
		通过URL传递信息。    个人理解：网页搜索东西的时候，输入的内容，转换为URL里面的字段
		ep：
			打开百度，搜索关键字'word'
			观察URL的变化，https://www.baidu.com/s?wd=word   wd字段即为搜索的内容
			若word是ASCII可识别字符，直接添加，否则，先quote，再添加。
			代码举例：
				import urllib.request
				keywd='word'
				(quote_keywd=urllib.request.quote(keywd))
				url='https://www.baidu.com/s?wd='+keywd(或者quote_keywd)
				...
	POST请求：
		向服务器提交数据。  登陆的时候经常用,需要浏览器的模拟
		ep：
			右键网页，查看源文件，找到form表单部分
			代码举例：
				import urllib.request
				import urllib.parse
				url=''
				postdata=urllib.parse.urlencode({
				'name':'xxx'
				'password':'xxx'
				}).encode('utf-8')   字典部分，依据form表单来写
				req=urllib.request.Request(url,postdata)
				...
		
		post请求用到cookie：
			假如用http协议(无状态协议，即无法维持会话状态的协议)登陆进了网站，当我们访问改网站里面的其他网页时，该状态会消失，我们需要重新登陆。
			只要网页涉及更新，就要不断的登陆，很麻烦。
			用cookie可以把登陆的会话信息保存下来。(cookie保存在客户端，session保存在服务器端(还是要用到cookie，不深究session))
			代码示例：
				import urllib.request
				import urllib.parse
				import http.cookiejar
				url=''
				postdata=urllib.parse.urlencode({
				'':''
				'':''
				}).encode('utf-8')
				req=urllib.request.Request(url,postdata)
				req.add_header('':'')
				
				cjar=http.cookiejar.CookieJar()
				opener=urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cjar))
				urllib.request.install_opener(opener)
				
				url2=''
				...
				
6.	代理服务器：
		完整的格式，"代理IP地址:端口号"
		一个代理服务器:
			def use_proxy(proxy_addr,url):
				import urllib.request
				proxy=urllib.request.ProxyHandler({'http':proxy_addr})   设置代理服务器信息，格式为:urllib.request.ProxyHandler({'http':代理服务器地址})
				opener=urllib.request.build_opener(proxy,urllib.request.HTTPHandler)
				urllib.request.install_opener(opener)		创建全局默认的opener对象，后面的比如urlopen，可以直接用了
				data=urllib.request.urlopen(url).read().decode('utf-8')
				return data
		多个代理服务器：

7.	DebugLog
		开启调试日志代码：
			import urllib.request
			httphd=urllib.request.HTTPHandler(debuglevel=1)
			httpshd=urllib.request.HTTPSHandler(debuglevel=1)
			opener=urllib.request.build_opener(httphd,httpshd)
			urllib.request.install_opener(opener)

8.	URLError
		HTTPError是URLError的子类
		常见的状态码及其含义：
			200 OK(一切正常)
			301 Moved Permanently(重定向到新的URL，永久性)
			302 Found(重定向到临时的URL，非永久性)
			304 Not Modified(请求的资源未更新)
			400 Bad Request(非法请求)
			401 Unauthorized(请求未经授权)
			403 Forbidden(禁止访问)
			404 Not Found(没有找到对应的页面)
			500 (服务器内部出现错误)
			501 (服务器不支持实现请求所需要的功能)
		import urllib.request
		import urllib.error
		try:
			urllib.request.urlopen(url)
		except urllib.error.URLError as e:
			if hasattr(e,'code'):
				print(e.code)
			if hasattr(e,'reason'):
				print(e.reason)
